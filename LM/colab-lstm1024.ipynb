{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6I1hS2YMS6I",
        "outputId": "3e957cb9-e88d-475f-eeac-efd1ced31bed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-01 21:24:29.939779: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-01 21:24:30.906751: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "index('<EOS>') = 3\n",
            "The Queen's messages to those celebrating their 90th birthdays on 21 April\n",
            " eeeooosss\n",
            "[2, 182, 1000, 6, 40, 585, 42, 2095, 2096, 22, 1166, 531, 3]\n",
            "[[2], [2, 182], [2, 182, 1000], [2, 182, 1000, 6], [2, 182, 1000, 6, 40], [2, 182, 1000, 6, 40, 585], [2, 182, 1000, 6, 40, 585, 42], [2, 182, 1000, 6, 40, 585, 42, 2095], [2, 182, 1000, 6, 40, 585, 42, 2095, 2096], [2, 182, 1000, 6, 40, 585, 42, 2095, 2096, 22]]\n",
            "[2, 182, 1000]\n",
            "6\n",
            "[   0    0    0    0    0    0    0    2  182 1000]\n",
            "[0. 0. 0. ... 0. 0. 0.]\n",
            "2023-05-01 21:24:32.972201: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-01 21:24:33.524186: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-01 21:24:33.524494: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-01 21:24:33.525524: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-01 21:24:33.525833: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-01 21:24:33.526076: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-01 21:24:36.155632: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-01 21:24:36.155928: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-01 21:24:36.156125: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-01 21:24:36.156264: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2023-05-01 21:24:36.156308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13678 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 128)         718848    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 1024)              4722688   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 5616)              5756400   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,197,936\n",
            "Trainable params: 11,197,936\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "2023-05-01 21:24:38.184826: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1237272192 exceeds 10% of free system memory.\n",
            "2023-05-01 21:24:39.512124: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1237272192 exceeds 10% of free system memory.\n",
            "Epoch 1/20\n",
            "2023-05-01 21:24:43.675171: W tensorflow/core/common_runtime/type_inference.cc:339] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:\n",
            "type_id: TFT_OPTIONAL\n",
            "args {\n",
            "  type_id: TFT_PRODUCT\n",
            "  args {\n",
            "    type_id: TFT_TENSOR\n",
            "    args {\n",
            "      type_id: TFT_INT32\n",
            "    }\n",
            "  }\n",
            "}\n",
            " is neither a subtype nor a supertype of the combined inputs preceding it:\n",
            "type_id: TFT_OPTIONAL\n",
            "args {\n",
            "  type_id: TFT_PRODUCT\n",
            "  args {\n",
            "    type_id: TFT_TENSOR\n",
            "    args {\n",
            "      type_id: TFT_FLOAT\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "\twhile inferring type of node 'cond_40/output/_23'\n",
            "2023-05-01 21:24:47.322438: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f906c893a50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2023-05-01 21:24:47.322493: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2023-05-01 21:24:47.475344: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "2023-05-01 21:24:48.146777: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8700\n",
            "2023-05-01 21:24:48.667835: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "216/216 [==============================] - 37s 133ms/step - loss: 6.7730 - accuracy: 0.0663\n",
            "Epoch 2/20\n",
            "216/216 [==============================] - 14s 64ms/step - loss: 6.1229 - accuracy: 0.1039\n",
            "Epoch 3/20\n",
            "216/216 [==============================] - 12s 57ms/step - loss: 5.7580 - accuracy: 0.1262\n",
            "Epoch 4/20\n",
            "216/216 [==============================] - 13s 59ms/step - loss: 5.4061 - accuracy: 0.1487\n",
            "Epoch 5/20\n",
            "216/216 [==============================] - 11s 51ms/step - loss: 4.9953 - accuracy: 0.1714\n",
            "Epoch 6/20\n",
            "216/216 [==============================] - 10s 47ms/step - loss: 4.5096 - accuracy: 0.1994\n",
            "Epoch 7/20\n",
            "216/216 [==============================] - 11s 50ms/step - loss: 3.9515 - accuracy: 0.2501\n",
            "Epoch 8/20\n",
            "216/216 [==============================] - 11s 51ms/step - loss: 3.3503 - accuracy: 0.3333\n",
            "Epoch 9/20\n",
            "216/216 [==============================] - 10s 44ms/step - loss: 2.7660 - accuracy: 0.4401\n",
            "Epoch 10/20\n",
            "216/216 [==============================] - 11s 49ms/step - loss: 2.2411 - accuracy: 0.5506\n",
            "Epoch 11/20\n",
            "216/216 [==============================] - 11s 49ms/step - loss: 1.7899 - accuracy: 0.6594\n",
            "Epoch 12/20\n",
            "216/216 [==============================] - 10s 48ms/step - loss: 1.4234 - accuracy: 0.7523\n",
            "Epoch 13/20\n",
            "216/216 [==============================] - 12s 55ms/step - loss: 1.1474 - accuracy: 0.8159\n",
            "Epoch 14/20\n",
            "216/216 [==============================] - 11s 49ms/step - loss: 0.9543 - accuracy: 0.8530\n",
            "Epoch 15/20\n",
            "216/216 [==============================] - 10s 48ms/step - loss: 0.8307 - accuracy: 0.8722\n",
            "Epoch 16/20\n",
            "216/216 [==============================] - 10s 44ms/step - loss: 0.7567 - accuracy: 0.8816\n",
            "Epoch 17/20\n",
            "216/216 [==============================] - 10s 48ms/step - loss: 0.7086 - accuracy: 0.8868\n",
            "Epoch 18/20\n",
            "216/216 [==============================] - 11s 49ms/step - loss: 0.6738 - accuracy: 0.8904\n",
            "Epoch 19/20\n",
            "216/216 [==============================] - 10s 48ms/step - loss: 0.6474 - accuracy: 0.8934\n",
            "Epoch 20/20\n",
            "216/216 [==============================] - 10s 48ms/step - loss: 0.6261 - accuracy: 0.8956\n",
            "the quick brown is that our countries have\n",
            "GEN max prob\n",
            "i am pleased that prince harry is able to present a new colour to the royal air force regiment in this your 75th anniversary year on my behalf as your\n",
            "GEN sampled\n",
            "plans conductor presidents ladies and gentlemen thank you for your most generous speech\n",
            "GEN sampled best 50\n",
            "when i came to the throne cardiff's port had already begun in our thoughts with the wider service\n",
            "Perplexity: 85165.30969077359\n"
          ]
        }
      ],
      "source": [
        "!python toyLM_LSTM.py"
      ]
    }
  ]
}